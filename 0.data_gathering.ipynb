{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrapper for \"Izquierda Diario\"\n",
    "It can work simply based on bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "base_url = \"https://www.laizquierdadiario.com/Economia/\"\n",
    "\n",
    "def parse_url(url):\n",
    "    response = requests.get(url)\n",
    "    content = response.content\n",
    "    parsed_response = BeautifulSoup(content, \"html.parser\")\n",
    "    return parsed_response\n",
    "\n",
    "soup = parse_url(base_url)\n",
    "here = soup.find('strong', {'class':\"on\"})\n",
    "\n",
    "flyer = []\n",
    "headline = []\n",
    "url = []\n",
    "\n",
    "date = []\n",
    "lead = []\n",
    "body = []\n",
    "\n",
    "i = 0\n",
    "n_news = 0\n",
    "\n",
    "# arbitrary stop at 300 iterations\n",
    "while int(here.text) < 600:\n",
    "    here = soup.find('strong', {'class':\"on\"})\n",
    "    print(f\"{i + 1}º page, articles from {int(here.text)} to {soup.find_all('a', {'class':'lien_pagination'})[i].text}\")\n",
    "\n",
    "    # strict selection\n",
    "    news = soup.select(\"div[class=noticia]\")\n",
    "    n_news += len(news)\n",
    "    print('News collected:', n_news)\n",
    "\n",
    "    # parse while reading page source\n",
    "    for p_news in news:\n",
    "        link = base_url + p_news.find('a')['href']\n",
    "\n",
    "        url.append(link)\n",
    "        if p_news.find('h4') is not None:\n",
    "            flyer.append(p_news.find('h4').text)\n",
    "        else:\n",
    "            flyer.append('')\n",
    "        headline.append(p_news.find('a').text)\n",
    "\n",
    "        web = requests.get(link)\n",
    "        article = BeautifulSoup(web.content, 'html.parser')\n",
    "        \n",
    "        if article.find('div', {'class': 'header-articulo'}) is not None:\n",
    "            if article.find('div', {'class': 'header-articulo'}).find('p') is not None:        \n",
    "                lead.append(article.find('div', {'class': 'header-articulo'}).find('p').text)\n",
    "            else:\n",
    "                lead.append('')\n",
    "            date.append(article.find('span', {'style': 'color: #364b67;'}).text)\n",
    "\n",
    "            texto = str()\n",
    "            for paragraph in article.find('div', {'class': 'articulo'}).findAll('p')[:-5]:\n",
    "                texto = texto + paragraph.text\n",
    "            body.append(texto)\n",
    "        else:\n",
    "            body.append('')\n",
    "            date.append('')\n",
    "            lead.append('')\n",
    "\n",
    "    # go to next page\n",
    "    next_pag_url = base_url + soup.find_all('a', {'class':\"lien_pagination\"})[i]['href']\n",
    "    soup = parse_url(next_pag_url)\n",
    "    if i < 5:\n",
    "        i += 1\n",
    "    print('------------------------------------------')\n",
    "\n",
    "# make dict with lists\n",
    "data = {\n",
    "    'date': date,\n",
    "    'flyer': flyer,\n",
    "    'lead': lead,\n",
    "    'headline': headline,\n",
    "    'body': body,\n",
    "    'url': url\n",
    "}\n",
    "\n",
    "# save everything to a csv file\n",
    "pd.DataFrame(data).to_csv('data/izq_econ_news.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrapper for \"Derecha Diario\"\n",
    "It requires the use of selenium and driver-based navigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "\n",
    "from time import time, strftime, sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "\n",
    "from time import time, strftime, sleep\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "url = \"https://derechadiario.com.ar/economia\"\n",
    "driver = \"./drivers/geckodriver\"\n",
    "delay = 3\n",
    "\n",
    "options = Options()\n",
    "options.headless = True\n",
    "options.page_load_strategy = 'normal'\n",
    "\n",
    "with webdriver.Firefox(executable_path = driver, options=options) as browser:\n",
    "\n",
    "    browser.get(url)\n",
    "    print('url',browser.current_url)\n",
    "    \n",
    "    start = time()\n",
    "    print('Start time:', strftime(\"%H:%M:%S\"))\n",
    "\n",
    "    count=0\n",
    "\n",
    "    while True:\n",
    "        count += 1\n",
    "        button = browser.find_element(By.XPATH, '//*[@id=\"__next\"]/div/div[3]/div/button')\n",
    "        if button.text != 'Ver más de Economía':\n",
    "            print('No more to load')\n",
    "            break        \n",
    "        browser.execute_script('arguments[0].scrollIntoView(true);', button)\n",
    "        sleep(3)\n",
    "\n",
    "        elements = browser.find_elements(By.TAG_NAME,'article')\n",
    "        print(f'{count} page(s) loaded with {len(elements)} articles')\n",
    "\n",
    "        button.click()\n",
    "        sleep(3)\n",
    "    \n",
    "    # save page to parse with bs4\n",
    "    soup = BeautifulSoup(browser.page_source, 'html.parser')\n",
    "    news = soup.findAll('a', {'class': 'jsx-1028219248'})\n",
    "    print(f'{len(news)} articles founded')\n",
    "\n",
    "    end = time()\n",
    "    print('End of loading:', strftime(\"%H:%M:%S\"))\n",
    "    print('Total time:', (end - start)/60)\n",
    "\n",
    "    browser.quit()\n",
    "\n",
    "    date = []\n",
    "    flyer = []\n",
    "    headline = []\n",
    "    lead = []\n",
    "    body = []\n",
    "    link = []\n",
    "\n",
    "    # parse and classify\n",
    "    for i, p_news in enumerate(news):\n",
    "\n",
    "        url = 'https://derechadiario.com.ar' + p_news['href']\n",
    "        link.append(url)\n",
    "        date.append(p_news.findAll('p')[2].text)\n",
    "        headline.append(p_news.find('h1').text)\n",
    "        lead.append(p_news.findAll('p')[1].text)\n",
    "        flyer.append(p_news.findAll('p')[0].text)\n",
    "\n",
    "        web = requests.get(url)\n",
    "        article = BeautifulSoup(web.content, 'html.parser')\n",
    "        content = article.find('div', {'class':\"jsx-2701897770 editor body\"})\n",
    "        body.append(content.text)\n",
    "\n",
    "    data = {\n",
    "        'date': date,\n",
    "        'flyer': flyer,\n",
    "        'title': headline,\n",
    "        'lead': lead,\n",
    "        'body': body,\n",
    "        'url': link\n",
    "    }\n",
    "\n",
    "    # save to a csv file\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv('derecha_econ_news.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
